# ============================================================
# ğŸš€ QWEN3-4B-INSTRUCT-2507 - NAHUATL EXPERT TRAINING V5
# ============================================================
# ConfiguraciÃ³n: Transformers + PEFT + RSLoRA + Dual T4 + W&B
# Dataset: 70k Balanceado (59k Gold + 11k DPO sobremuestreado 10x)
# Kaggle GPU: T4 x2 (30GB VRAM Total)
# Modelo: 4B Parameters â€” SIN cuantizaciÃ³n (bf16 nativo)
# ============================================================

import os
import subprocess
import sys
import gc
import shutil
import json
from pathlib import Path
from datetime import datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“¦ SECCIÃ“N 0: INSTALACIÃ“N DE DEPENDENCIAS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("ğŸ“¦ Actualizando librerÃ­as crÃ­ticas...")
subprocess.check_call([
    sys.executable, "-m", "pip", "install", "-q", "-U",
    "trl", "transformers>=4.51.0", "accelerate", "peft",
    "bitsandbytes", "wandb", "huggingface_hub"
])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”§ SECCIÃ“N 1: CONFIGURACIÃ“N DEL ENTORNO
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'
os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
import pandas as pd
import wandb
from datasets import Dataset
from huggingface_hub import login, HfApi
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, SFTConfig
from kaggle_secrets import UserSecretsClient

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”‘ SECCIÃ“N 2: AUTENTICACIÃ“N
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

user_secrets = UserSecretsClient()
hf_token = user_secrets.get_secret("hf_token")
login(token=hf_token)

# Obtener username de HF para repo de checkpoints
from huggingface_hub import HfApi
api = HfApi()
user_info = api.whoami(token=hf_token)
username = user_info["name"]

os.environ["WANDB_API_KEY"] = user_secrets.get_secret("wandb_api_key")
wandb.init(
    project="qwen3-4b-nahuatl-v5",
    name=f"run-4b-instruct-{datetime.now().strftime('%m%d-%H%M')}",
    tags=["qwen3-4b", "nahuatl", "sft", "v5"]
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“‚ SECCIÃ“N 3: DETECCIÃ“N DE MODELO Y DATASET
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ğŸ” Rutas de modelo (Kaggle Models vs HuggingFace)
MODEL_ID = "Qwen/Qwen3-4B-Instruct-2507"
LOCAL_PATHS = [
    "/kaggle/input/qwen3-4b-instruct-2507/transformers/default/1",
    "/kaggle/input/qwen3-4b-instruct-2507",
    "/kaggle/input/wowfattie-qwen3-4b-instruct-2507",
]

model_path = MODEL_ID  # Default: descargar de HF
for path in LOCAL_PATHS:
    if os.path.exists(path):
        # Verificar que tiene archivos de modelo
        files = os.listdir(path) if os.path.isdir(path) else []
        if any(f.endswith(('.safetensors', '.bin', 'config.json')) for f in files):
            model_path = path
            print(f"âœ… Modelo local detectado: {path}")
            break
        # Buscar subdirectorios
        for sub in os.listdir(path):
            subpath = os.path.join(path, sub)
            if os.path.isdir(subpath):
                subfiles = os.listdir(subpath)
                if any(f.endswith(('.safetensors', '.bin', 'config.json')) for f in subfiles):
                    model_path = subpath
                    print(f"âœ… Modelo local detectado: {subpath}")
                    break
        if model_path != MODEL_ID:
            break

if model_path == MODEL_ID:
    print(f"âš ï¸ Modelo local no detectado. Descargando de HF: {MODEL_ID}")

# ğŸ¯ Dataset
DATA_PATH = "/kaggle/input/nahuatl-balanced-corpus-70k/train_balanced_v2.parquet"
if not os.path.exists(DATA_PATH):
    # Intentar paths alternativos
    alt_paths = [
        "/kaggle/input/nahuatl-balanced-corpus/train_balanced_v2.parquet",
        "/kaggle/input/train-balanced-v2/train_balanced_v2.parquet",
    ]
    for alt in alt_paths:
        if os.path.exists(alt):
            DATA_PATH = alt
            break
    else:
        raise FileNotFoundError(
            f"âŒ Dataset no encontrado. IntentÃ©:\n"
            f"  - {DATA_PATH}\n"
            f"  - {chr(10).join(alt_paths)}\n"
            f"Agrega 'nahuatl-balanced-corpus-70k' como dataset en el notebook."
        )

print(f"ğŸ“‚ Modelo: {model_path}")
print(f"ğŸ“‚ Dataset: {DATA_PATH}")

# ğŸ”„ CHECKPOINT ANTERIOR (para resumir entrenamiento)
RESUME_FROM_CHECKPOINT = None

# â”€â”€â”€â”€â”€â”€â”€â”€ DetecciÃ³n de checkpoints locales â”€â”€â”€â”€â”€â”€â”€â”€
CHECKPOINT_PATHS = [
    "/kaggle/input/qwen3-4b-nawatl-checkpoint/checkpoint-latest",
    "/kaggle/input/qwen3-4b-checkpoint",
]
for ckpt in CHECKPOINT_PATHS:
    if os.path.exists(ckpt):
        # Buscar el checkpoint mÃ¡s reciente
        if os.path.isdir(ckpt):
            subdirs = [d for d in os.listdir(ckpt) if d.startswith("checkpoint-")]
            if subdirs:
                latest = sorted(subdirs, key=lambda x: int(x.split("-")[1]))[-1]
                RESUME_FROM_CHECKPOINT = os.path.join(ckpt, latest)
            elif os.path.exists(os.path.join(ckpt, "adapter_config.json")):
                RESUME_FROM_CHECKPOINT = ckpt
        if RESUME_FROM_CHECKPOINT:
            print(f"ğŸ”„ Checkpoint local detectado: {RESUME_FROM_CHECKPOINT}")
            break

# â”€â”€â”€â”€â”€â”€â”€â”€ Si no hay checkpoint local, buscar en HF Hub â”€â”€â”€â”€â”€â”€â”€â”€
if not RESUME_FROM_CHECKPOINT:
    print("ğŸ†• No hay checkpoint local, verificando HuggingFace Hub...")
    
    try:
        from huggingface_hub import snapshot_download, list_repo_files
        
        # Usar mismo repo que el callback
        HF_REPO_ID = f"{username}/qwen3-4b-nawatl-v5-checkpoints"
        
        # Listar archivos para encontrar checkpoints
        try:
            all_files = list_repo_files(HF_REPO_ID, repo_type="model", token=hf_token)
            
            # Encontrar checkpoints (e.g., checkpoint-100/, checkpoint-200/)
            checkpoints_in_hub = set()
            for file_path in all_files:
                if file_path.startswith("checkpoint-"):
                    ckpt_name = file_path.split("/")[0]
                    checkpoints_in_hub.add(ckpt_name)
            
            if checkpoints_in_hub:
                # Ordenar por nÃºmero de step
                sorted_ckpts = sorted(
                    checkpoints_in_hub,
                    key=lambda x: int(x.split("-")[1])
                )
                latest_hub_ckpt = sorted_ckpts[-1]
                
                print(f"â˜ï¸ Checkpoint encontrado en HF Hub: {latest_hub_ckpt}")
                print(f"   Descargando desde {HF_REPO_ID}...")
                
                # Descargar solo el Ãºltimo checkpoint
                local_dir = f"/kaggle/working/hf_checkpoint"
                snapshot_download(
                    repo_id=HF_REPO_ID,
                    repo_type="model",
                    allow_patterns=[f"{latest_hub_ckpt}/**"],
                    local_dir=local_dir,
                    token=hf_token,
                )
                
                RESUME_FROM_CHECKPOINT = os.path.join(local_dir, latest_hub_ckpt)
                print(f"   âœ… Checkpoint descargado: {RESUME_FROM_CHECKPOINT}")
            else:
                print(f"   â„¹ï¸ No hay checkpoints en {HF_REPO_ID}")
                print(f"   ğŸ†• Entrenamiento desde cero")
        
        except Exception as e:
            print(f"   âš ï¸ Error consultando HF Hub: {e}")
            print(f"   ğŸ†• Entrenamiento desde cero")
    
    except ImportError:
        print("   âš ï¸ huggingface_hub no disponible para auto-descarga")
        print(f"   ğŸ†• Entrenamiento desde cero")

if not RESUME_FROM_CHECKPOINT:
    print("ğŸ†• Entrenamiento desde cero (sin checkpoint previo)")


OUTPUT_DIR = "./qwen3-4b-nawatl-v5"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¤– SECCIÃ“N 4: CARGAR MODELO (SIN CUANTIZACIÃ“N)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

gc.collect()
torch.cuda.empty_cache()

print("ğŸš€ Cargando Qwen3-4B-Instruct-2507 en bf16 NATIVO...")
print(f"   ğŸ’¡ Sin cuantizaciÃ³n â€” 4B params caben en T4x2 (~8GB)")

# Detectar si T4 soporta bf16 (Turing: parcial via Tensor Cores)
# Usamos fp16 que es nativamente soportado y mÃ¡s rÃ¡pido en T4
if torch.cuda.is_available():
    capability = torch.cuda.get_device_capability()
    gpu_name = torch.cuda.get_device_name()
    print(f"   ğŸ® GPU: {gpu_name} (compute {capability[0]}.{capability[1]})")
    n_gpus = torch.cuda.device_count()
    total_vram = sum(torch.cuda.get_device_properties(i).total_memory for i in range(n_gpus))
    print(f"   ğŸ’¾ GPUs: {n_gpus}x | VRAM Total: {total_vram / 1e9:.1f} GB")

    # T4 = compute 7.5 â†’ usar fp16 (Tensor Cores optimizados para fp16)
    # A100+ = compute 8.0+ â†’ usar bf16
    USE_BF16 = capability[0] >= 8
    USE_FP16 = not USE_BF16
    COMPUTE_DTYPE = torch.bfloat16 if USE_BF16 else torch.float16
    print(f"   âš¡ PrecisiÃ³n: {'bfloat16' if USE_BF16 else 'float16'}")
else:
    raise RuntimeError("âŒ No se detectÃ³ GPU. Activa T4x2 en Settings.")

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=COMPUTE_DTYPE,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True,
    attn_implementation="eager",  # Compatibilidad universal
)

# Verificar carga exitosa
total_params = sum(p.numel() for p in model.parameters())
trainable_before = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"   âœ… Modelo cargado: {total_params / 1e9:.2f}B parÃ¡metros")
print(f"   ğŸ“Š Memoria GPU usada: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ›ï¸ SECCIÃ“N 5: CONFIGURACIÃ“N LoRA (RSLoRA)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

peft_config = LoraConfig(
    r=32,                        # â¬†ï¸ MÃ¡s rank (4B permite mÃ¡s)
    lora_alpha=64,               # Mantener alpha = 2Ã—r
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    use_rslora=True,             # RSLoRA para estabilidad
)

print(f"ğŸ›ï¸ LoRA Config: r={peft_config.r}, alpha={peft_config.lora_alpha}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Š SECCIÃ“N 6: PREPARACIÃ“N DE DATOS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def formatting_prompts_func(example):
    """Formato ChatML compatible con Qwen3-4B-Instruct-2507."""
    return (
        f"<|im_start|>user\n"
        f"Traduce al NÃ¡huatl ClÃ¡sico:\n{example['es']}<|im_end|>\n"
        f"<|im_start|>assistant\n"
        f"{example['nah']}<|im_end|>"
    )

print(f"ğŸ“Š Cargando Dataset desde: {DATA_PATH}")
df = pd.read_parquet(DATA_PATH)
print(f"   âœ… {len(df):,} ejemplos cargados")

# Verificar columnas
assert 'es' in df.columns, f"âŒ Columna 'es' no encontrada. Columnas: {list(df.columns)}"
assert 'nah' in df.columns, f"âŒ Columna 'nah' no encontrada. Columnas: {list(df.columns)}"

# EstadÃ­sticas rÃ¡pidas
print(f"   ğŸ“ Longitud promedio ES: {df['es'].str.len().mean():.0f} chars")
print(f"   ğŸ“ Longitud promedio NAH: {df['nah'].str.len().mean():.0f} chars")

dataset = Dataset.from_pandas(df)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âš™ï¸ SECCIÃ“N 7: ARGUMENTOS DE ENTRENAMIENTO
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# CÃ¡lculos de steps
PER_DEVICE_BATCH = 4
GRAD_ACCUM = 4
EFFECTIVE_BATCH = PER_DEVICE_BATCH * GRAD_ACCUM * max(1, torch.cuda.device_count())
NUM_EPOCHS = 3
STEPS_PER_EPOCH = len(df) // EFFECTIVE_BATCH
TOTAL_STEPS = STEPS_PER_EPOCH * NUM_EPOCHS

print(f"\n{'='*60}")
print(f"âš™ï¸  CONFIGURACIÃ“N DE ENTRENAMIENTO V5")
print(f"{'='*60}")
print(f"ğŸ“‚ Dataset: {len(df):,} ejemplos")
print(f"ğŸ“ Ã‰pocas: {NUM_EPOCHS}")
print(f"ğŸ“¦ Batch: {PER_DEVICE_BATCH}/device Ã— {GRAD_ACCUM} accum Ã— {torch.cuda.device_count()} GPU = {EFFECTIVE_BATCH} efectivo")
print(f"â±ï¸  Steps por Ã©poca: ~{STEPS_PER_EPOCH:,}")
print(f"â±ï¸  Steps totales: ~{TOTAL_STEPS:,}")
print(f"ğŸ’¾ Checkpoints cada: 100 steps (guardando mejores 5)")
print(f"ğŸ”„ Resume: {RESUME_FROM_CHECKPOINT or 'Desde cero'}")
print(f"{'='*60}\n")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,

    # ğŸ“¦ Batch y Ã‰pocas
    per_device_train_batch_size=PER_DEVICE_BATCH,
    gradient_accumulation_steps=GRAD_ACCUM,
    num_train_epochs=NUM_EPOCHS,

    # ğŸ“ˆ Learning Rate
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    weight_decay=0.01,

    # ğŸ’¾ Checkpoints â€” cada 100 steps, guardar mejores 5
    save_strategy="steps",
    save_steps=100,
    save_total_limit=5,

    # ğŸ“Š Logging
    logging_steps=10,
    logging_first_step=True,
    report_to="wandb",

    # âš¡ PrecisiÃ³n â€” FP16 para T4 Tensor Cores
    fp16=USE_FP16,
    bf16=USE_BF16,

    # ğŸ§  OptimizaciÃ³n de memoria
    optim="adamw_torch_fused",    # MÃ¡s rÃ¡pido que paged_adamw_8bit para 4B
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},

    # ğŸ¯ RegularizaciÃ³n
    neftune_noise_alpha=5,
    max_grad_norm=1.0,

    # ğŸ”„ Resume
    resume_from_checkpoint=RESUME_FROM_CHECKPOINT,

    # ğŸ“ Misc
    dataloader_num_workers=2,
    remove_unused_columns=False,
    seed=42,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ‹ï¸ SECCIÃ“N 8: INICIALIZAR TRAINER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("ğŸ› ï¸ Configurando SFTTrainer...")

import inspect

potential_kwargs = {
    "model": model,
    "train_dataset": dataset,
    "peft_config": peft_config,
    "formatting_func": formatting_prompts_func,
    "args": training_args,
    "max_seq_length": 512,  # â¬†ï¸ 2x vs V4 (el modelo soporta 262K)
    "tokenizer": tokenizer,
    "processing_class": tokenizer,
    "dataset_text_field": None,
}

# Compatibilidad dinÃ¡mica con diferentes versiones de TRL
sig = inspect.signature(SFTTrainer.__init__)
valid_params = sig.parameters.keys()
trainer_kwargs = {k: v for k, v in potential_kwargs.items() if k in valid_params and v is not None}

print(f"âœ… ParÃ¡metros aceptados: {list(trainer_kwargs.keys())}")

# Limpieza pre-entrenamiento
gc.collect()
torch.cuda.empty_cache()

trainer = SFTTrainer(**trainer_kwargs)

# Verificar parÃ¡metros entrenables
trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)
all_params = sum(p.numel() for p in trainer.model.parameters())
print(f"ğŸ›ï¸ ParÃ¡metros entrenables: {trainable_params:,} / {all_params:,} ({100*trainable_params/all_params:.2f}%)")
print(f"ğŸ’¾ Memoria GPU post-setup: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”„ SECCIÃ“N 9: CALLBACK PARA GUARDAR CHECKPOINTS EN HF HUB
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Kaggle NO puede montar Google Drive directamente.
# En su lugar, subimos checkpoints a HuggingFace Hub automÃ¡ticamente
# para que sobrevivan entre sesiones.

from transformers import TrainerCallback

HF_REPO_ID = None  # Se configura abajo

class HubCheckpointCallback(TrainerCallback):
    """Sube cada checkpoint guardado a HuggingFace Hub para persistencia."""

    def __init__(self, repo_id, tokenizer, every_n_saves=1):
        self.repo_id = repo_id
        self.tokenizer = tokenizer
        self.api = HfApi()
        self.every_n_saves = every_n_saves
        self.save_count = 0

    def on_save(self, args, state, control, **kwargs):
        self.save_count += 1
        if self.save_count % self.every_n_saves != 0:
            return

        checkpoint_dir = os.path.join(args.output_dir, f"checkpoint-{state.global_step}")
        if not os.path.exists(checkpoint_dir):
            return

        try:
            print(f"\nâ˜ï¸  Subiendo checkpoint-{state.global_step} a HF Hub ({self.repo_id})...")

            # ğŸ”§ FIX: Corregir adapter_config.json antes de subir
            adapter_config_path = os.path.join(checkpoint_dir, "adapter_config.json")
            CANONICAL_MODEL_ID = "Qwen/Qwen3-4B-Instruct-2507"
            
            if os.path.exists(adapter_config_path):
                with open(adapter_config_path, "r") as f:
                    config = json.load(f)
                
                # Reemplazar path local de Kaggle con modelo canÃ³nico
                old_base = config.get("base_model_name_or_path", "")
                if "/kaggle/" in old_base or not old_base.startswith("Qwen/"):
                    config["base_model_name_or_path"] = CANONICAL_MODEL_ID
                    
                    with open(adapter_config_path, "w") as f:
                        json.dump(config, f, indent=2)
                    
                    print(f"   ğŸ”§ Corregido adapter_config.json: {old_base} â†’ {CANONICAL_MODEL_ID}")
            
            # ğŸ”§ FIX: Corregir README.md tambiÃ©n
            readme_path = os.path.join(checkpoint_dir, "README.md")
            if os.path.exists(readme_path):
                import re
                
                with open(readme_path, "r", encoding="utf-8") as f:
                    readme_content = f.read()
                
                # PatrÃ³n: buscar lÃ­nea "base_model: /kaggle/..." en frontmatter YAML
                pattern = r"(base_model:\s*)([^\n]+)"
                
                def replace_base_model(match):
                    prefix = match.group(1)
                    old_value = match.group(2).strip()
                    if "/kaggle/" in old_value or not old_value.startswith("Qwen/"):
                        return f"{prefix}{CANONICAL_MODEL_ID}"
                    return match.group(0)
                
                readme_fixed = re.sub(pattern, replace_base_model, readme_content)
                
                with open(readme_path, "w", encoding="utf-8") as f:
                    f.write(readme_fixed)
                
                print(f"   ğŸ”§ Corregido README.md metadata")


            # Subir archivos del checkpoint
            self.api.upload_folder(
                folder_path=checkpoint_dir,
                repo_id=self.repo_id,
                path_in_repo=f"checkpoint-{state.global_step}",
                repo_type="model",
                commit_message=f"Checkpoint step {state.global_step} | loss={state.log_history[-1].get('loss', 'N/A') if state.log_history else 'N/A'}",
            )

            # TambiÃ©n guardar un archivo de estado para saber cuÃ¡l es el Ãºltimo
            state_info = {
                "last_checkpoint_step": state.global_step,
                "epoch": state.epoch,
                "total_steps": state.max_steps,
                "timestamp": datetime.now().isoformat(),
                "loss": state.log_history[-1].get("loss") if state.log_history else None,
            }
            state_path = os.path.join(args.output_dir, "hub_state.json")
            with open(state_path, "w") as f:
                json.dump(state_info, f, indent=2)

            self.api.upload_file(
                path_or_fileobj=state_path,
                path_in_repo="hub_state.json",
                repo_id=self.repo_id,
                repo_type="model",
                commit_message=f"Update state: step {state.global_step}",
            )

            print(f"   âœ… Checkpoint-{state.global_step} subido exitosamente")
        except Exception as e:
            print(f"   âš ï¸ Error subiendo a Hub (no fatal): {e}")
            print(f"   ğŸ’¡ El checkpoint local sigue guardado en: {checkpoint_dir}")

# Configurar persistencia en HF Hub
try:
    api = HfApi()
    user_info = api.whoami()
    username = user_info["name"]
    HF_REPO_ID = f"{username}/qwen3-4b-nawatl-v5-checkpoints"

    # Crear repo si no existe
    try:
        api.create_repo(HF_REPO_ID, repo_type="model", private=True, exist_ok=True)
        print(f"â˜ï¸  Persistencia HF Hub activada: {HF_REPO_ID}")
        print(f"   ğŸ“ Ver en: https://huggingface.co/{HF_REPO_ID}")

        # Agregar callback al trainer
        hub_callback = HubCheckpointCallback(
            repo_id=HF_REPO_ID,
            tokenizer=tokenizer,
            every_n_saves=1,  # Subir CADA checkpoint (cada 100 steps)
        )
        trainer.add_callback(hub_callback)
    except Exception as e:
        print(f"âš ï¸ No se pudo crear repo HF: {e}")
        print("   ğŸ’¡ Los checkpoints se guardarÃ¡n solo localmente.")
except Exception as e:
    print(f"âš ï¸ HF Hub no disponible: {e}")
    print("   ğŸ’¡ Los checkpoints se guardarÃ¡n solo localmente.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”¥ SECCIÃ“N 10: Â¡ENTRENAR!
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n" + "ğŸ”¥" * 30)
print("ğŸ”¥ INICIANDO ENTRENAMIENTO V5 - QWEN3-4B NÃHUATL EXPERT")
print("ğŸ”¥" * 30 + "\n")
print("ğŸ“Š Monitoreo en tiempo real:")
print(f"   W&B: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}")
print(f"   HF:  https://huggingface.co/{HF_REPO_ID or 'N/A'}")
print(f"   ğŸ’¾ Checkpoints locales: {OUTPUT_DIR}/checkpoint-XXXX")
print(f"\nâ° Tiempo estimado: ~{TOTAL_STEPS * 10 / 3600:.1f}h ({TOTAL_STEPS} steps Ã— ~10s/step)")
print(f"   Con 12h Kaggle: ~{12 * 3600 / 10:.0f} steps por sesiÃ³n\n")

trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ’¾ SECCIÃ“N 11: GUARDAR MODELO FINAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n" + "=" * 60)
print("âœ… ENTRENAMIENTO COMPLETADO")
print("=" * 60)

FINAL_DIR = f"{OUTPUT_DIR}_final"
print(f"ğŸ’¾ Guardando adaptadores finales en: {FINAL_DIR}")
trainer.model.save_pretrained(FINAL_DIR)
tokenizer.save_pretrained(FINAL_DIR)

# Subir modelo final a HF Hub
if HF_REPO_ID:
    try:
        print(f"â˜ï¸ Subiendo modelo FINAL a HF Hub...")
        api.upload_folder(
            folder_path=FINAL_DIR,
            repo_id=HF_REPO_ID,
            path_in_repo="final",
            repo_type="model",
            commit_message="ğŸ‰ Modelo final entrenado",
        )
        print(f"   âœ… Modelo final subido a: https://huggingface.co/{HF_REPO_ID}/tree/main/final")
    except Exception as e:
        print(f"   âš ï¸ Error subiendo final: {e}")

# Empaquetar como ZIP para descarga directa
print("ğŸ“¦ Empaquetando modelo final como ZIP...")
shutil.make_archive(f"nawatl_qwen3_4b_final", "zip", FINAL_DIR)
print(f"   ğŸ“¥ Descarga: /kaggle/working/nawatl_qwen3_4b_final.zip")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§ª SECCIÃ“N 12: BENCHMARK RÃPIDO POST-ENTRENAMIENTO
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n" + "=" * 60)
print("ğŸ§ª BENCHMARK RÃPIDO POST-ENTRENAMIENTO")
print("=" * 60)

test_cases = [
    "El sol calienta la tierra",
    "Los niÃ±os juegan en el rÃ­o",
    "La lluvia cae suavemente sobre las montaÃ±as",
    "Mi corazÃ³n estÃ¡ lleno de alegrÃ­a",
    "La flor mÃ¡s bella crece en el jardÃ­n",
]

model.eval()
with torch.no_grad():
    for text in test_cases:
        prompt = (
            f"<|im_start|>user\n"
            f"Traduce al NÃ¡huatl ClÃ¡sico:\n{text}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            temperature=0.7,
            top_p=0.8,
            top_k=20,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        print(f"\n  ğŸ‡ªğŸ‡¸ {text}")
        print(f"  ğŸ‡²ğŸ‡½ {response.strip()}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Š SECCIÃ“N 13: RESUMEN FINAL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\n" + "=" * 60)
print("ğŸ“Š RESUMEN FINAL")
print("=" * 60)
print(f"ğŸ¤– Modelo: Qwen3-4B-Instruct-2507")
print(f"ğŸ“‚ Dataset: {len(df):,} ejemplos")
print(f"ğŸ“ Ã‰pocas completadas: {NUM_EPOCHS}")
print(f"ğŸ›ï¸ LoRA: r={peft_config.r}, alpha={peft_config.lora_alpha}")
print(f"ğŸ’¾ Modelo local: {FINAL_DIR}")
print(f"â˜ï¸ Modelo HF Hub: https://huggingface.co/{HF_REPO_ID or 'N/A'}")
print(f"ğŸ“Š W&B: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}")
print(f"\nğŸ¯ PrÃ³ximos pasos:")
print(f"   1. Descargar nawatl_qwen3_4b_final.zip")
print(f"   2. Ejecutar benchmark completo CHRF/BLEU")
print(f"   3. Si CHRF > 40: Iniciar Fase 2 (DPO)")
print(f"   4. Si necesitas continuar: sube checkpoint como dataset")
print("=" * 60)

wandb.finish()
print("\nâœ¨ Â¡Entrenamiento V5 completado exitosamente!")

