# Airflow DAGs - CORC-NAH

This directory contains Apache Airflow DAGs for orchestrating the CORC-NAH ETL pipeline.

Airflow is an **open-source** alternative to proprietary orchestrators like Control-M, providing:
- Visual DAG representation
- SLA monitoring
- Retry/error handling
- Parallel task execution
- Web UI for monitoring

---

## üìä DAGs

### `corc_nah_etl_dag.py`

Complete Medallion architecture pipeline:

```mermaid
graph LR
    BRONZE[Bronze Ingestion] --> SILVER[Silver Normalization]
    SILVER --> DIAMOND[Diamond Dedup]
    DIAMOND --> TRAIN[Gold Train]
    DIAMOND --> VAL[Gold Val]
    DIAMOND --> TEST[Gold Test]
    TRAIN --> QC[Quality Check]
    VAL --> QC
    TEST --> QC
    QC --> S3[Publish S3]
    S3 --> HF[Publish HF]
    HF --> CLEAN[Cleanup]
```

**Schedule**: Daily at 02:00 UTC  
**SLA**: 2 hours  
**Retries**: 3 with exponential backoff

**Features**:
- ‚úÖ Parallel execution (train/val/test splits)
- ‚úÖ Email alerts on failure
- ‚úÖ Great Expectations quality checks
- ‚úÖ S3 + HuggingFace publishing
- ‚úÖ Automatic cleanup

---

## üöÄ Local Setup

### Prerequisites

- Docker & Docker Compose
- 8 GB RAM minimum

### Quick Start

```bash
# Navigate to project root
cd corc-nah-enterprise/

# Start Airflow (all services)
docker-compose up -d

# Check status
docker-compose ps

# Access web UI
open http://localhost:8080

# Login credentials
# Username: admin
# Password: admin
```

### Manual Setup (without Docker)

```bash
# Install Airflow
pip install apache-airflow==2.8.0

# Initialize database
export AIRFLOW_HOME=$(pwd)/airflow_home
airflow db init

# Create admin user
airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com

# Start scheduler (background)
airflow scheduler &

# Start webserver
airflow webserver --port 8080
```

Access UI: http://localhost:8080

---

## üìÅ Project Structure

```
airflow_dags/
‚îú‚îÄ‚îÄ corc_nah_etl_dag.py       # Main ETL DAG
‚îú‚îÄ‚îÄ README.md                  # This file
‚îî‚îÄ‚îÄ (future DAGs)              # Additional orchestration workflows
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

Set in `.env` or Docker Compose:

```bash
# Airflow Core
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/airflow_dags

# Email Alerts (optional)
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_USER=your-email@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=your-app-password

# AWS (for S3 publishing)
AWS_ACCESS_KEY_ID=your-key
AWS_SECRET_ACCESS_KEY=your-secret
AWS_DEFAULT_REGION=us-east-1
```

### Connections

Configure in Airflow UI ‚Üí Admin ‚Üí Connections:

1. **AWS**  
   - Conn ID: `aws_default`
   - Conn Type: `Amazon Web Services`
   - Extra: `{"region_name": "us-east-1"}`

2. **PostgreSQL** (example for connectors)  
   - Conn ID: `postgres_production`
   - Conn Type: `Postgres`
   - Host: `prod-db.company.com`
   - Schema: `analytics`
   - Login/Password: (from vault)

---

## üîÑ Running DAGs

### Via Web UI

1. Navigate to http://localhost:8080
2. Enable DAG: Toggle `corc_nah_etl_pipeline` to ON
3. Trigger manually: Click ‚ñ∂Ô∏è button
4. Monitor: Click DAG name ‚Üí Graph view

### Via CLI

```bash
# List DAGs
docker-compose exec airflow-webserver airflow dags list

# Test specific task
docker-compose exec airflow-webserver \
    airflow tasks test corc_nah_etl_pipeline bronze_ingestion 2026-02-03

# Trigger full DAG
docker-compose exec airflow-webserver \
    airflow dags trigger corc_nah_etl_pipeline

# Check logs
docker-compose exec airflow-webserver \
    airflow tasks logs corc_nah_etl_pipeline bronze_ingestion 2026-02-03
```

---

## üìä Comparison: Airflow vs Jenkins vs Control-M

| Feature | Control-M | Jenkins | Airflow |
|---------|-----------|---------|---------|
| **Cost** | ‚ùå $50k+/year | ‚úÖ Free (open-source) | ‚úÖ Free (open-source) |
| **UI** | ‚úÖ Enterprise-grade | ‚ö†Ô∏è Basic | ‚úÖ Modern, intuitive |
| **DAG Visualization** | ‚úÖ Native | ‚ö†Ô∏è Plugins only | ‚úÖ Native |
| **Scheduling** | ‚úÖ Advanced cron | ‚úÖ Cron | ‚úÖ Cron + dynamic |
| **Data-focused** | ‚ö†Ô∏è General purpose | ‚ùå CI/CD-focused | ‚úÖ **Built for ETL** |
| **Dependencies** | ‚úÖ Complex chains | ‚ö†Ô∏è Pipelines | ‚úÖ DAG-native |
| **SLA Monitoring** | ‚úÖ Yes | ‚ö†Ô∏è Manual | ‚úÖ Built-in |
| **Cloud Integration** | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Manual | ‚úÖ Native (AWS, GCP, Azure) |
| **Community** | ‚ùå Proprietary | ‚úÖ Large | ‚úÖ **Very active** |
| **Learning Curve** | Medium | Low | Medium |

**Verdict**: Airflow is the **best open-source choice** for data pipelines.

---

## üß™ Testing DAGs

### Validate DAG syntax

```bash
# Python syntax
python airflow_dags/corc_nah_etl_dag.py

# Airflow DAG validation
airflow dags test corc_nah_etl_pipeline 2026-02-03
```

### Test individual tasks

```bash
# Test Bronze ingestion
airflow tasks test corc_nah_etl_pipeline bronze_ingestion 2026-02-03

# Test Python operator
airflow tasks test corc_nah_etl_pipeline silver_normalization 2026-02-03
```

---

## üìà Monitoring

### Key Metrics

- **DAG Run Duration**: Should be <2h (SLA)
- **Task Success Rate**: Should be >98%
- **Data Volume**: Track input/output records per layer

### Airflow Metrics

Access via UI or REST API:

```python
# Example: Get DAG run history
from airflow.api.client.local_client import Client

client = Client(None, None)
dag_runs = client.get_dag_runs('corc_nah_etl_pipeline')

for run in dag_runs:
    print(f"{run['execution_date']}: {run['state']}")
```

### Log Aggregation

For production, integrate with ELK/Splunk:

```yaml
# docker-compose.yml
volumes:
  - ./logs:/opt/airflow/logs  # Mount logs for external processing
```

---

## üöÄ Production Deployment

### Option 1: Docker Swarm

```bash
# Deploy to Swarm cluster
docker stack deploy -c docker-compose.prod.yml corc-nah-airflow
```

### Option 2: Kubernetes (Helm)

```bash
# Add Airflow Helm repo
helm repo add apache-airflow https://airflow.apache.org
helm repo update

# Install
helm install corc-nah-airflow apache-airflow/airflow \
    --set dags.gitSync.enabled=true \
    --set dags.gitSync.repo=https://github.com/saidmoreno808/nahuatl-data-pipeline \
    --set dags.gitSync.branch=main \
    --set dags.gitSync.subPath=airflow_dags
```

### Option 3: Managed Service

- **AWS**: Amazon Managed Workflows for Apache Airflow (MWAA)
- **GCP**: Cloud Composer
- **Azure**: Azure Data Factory (Airflow integration)

---

## üîê Security Best Practices

1. **Secrets Management**: Use Airflow Connections, not hardcoded credentials
2. **RBAC**: Enable role-based access control in production
3. **Network**: Run Airflow behind VPN/firewall
4. **Encryption**: Enable Fernet key for secret encryption

```bash
# Generate Fernet key
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"

# Set in airflow.cfg
[core]
fernet_key = your-generated-key
```

---

## üìö Resources

- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Best Practices Guide](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Airflow Summit](https://www.airflowsummit.org/) - Annual conference
- [Astronomer.io](https://www.astronomer.io/) - Managed Airflow provider

---

## ü§ù Contributing

To add new DAGs:

1. Create file in `airflow_dags/`
2. Import from `airflow` package
3. Define DAG with `with DAG(...) as dag:`
4. Test locally: `airflow dags test <dag_id> <execution_date>`
5. Deploy: Git push (auto-synced in production)

**DAG naming convention**: `{project}_{purpose}_dag.py`

---

## üìÑ License

MIT License - See [LICENSE](../LICENSE)
